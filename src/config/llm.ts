import { ChatOpenAI } from '@langchain/openai';
import { ChatGoogleGenerativeAI } from '@langchain/google-genai';
import { JsonOutputParser } from '@langchain/core/output_parsers';
import { PromptTemplate } from '@langchain/core/prompts';
import { logger } from '../utils/logger.js';
import env from './environment.js';

export interface LLMProvider {
  name: string;
  model: string;
  client: any;
  isConfigured: boolean;
  priority: number;
}

export interface LLMConfig {
  temperature?: number;
  maxTokens?: number;
  timeout?: number;
  structuredOutput?: boolean;
  outputSchema?: any;
}

/**
 * LLM Manager con sistema de fallback din√°mico basado en configuraci√≥n
 */
class LLMManager {
  private providers: LLMProvider[] = [];
  private maxRetries = 3;
  private retryDelay = 1000; // 1 second

  constructor() {
    this.initializeProviders();
  }

  private initializeProviders() {
    logger.info('Inicializando proveedores LLM disponibles...');

    // Google Gemini (Priority 1) - Solo si est√° configurado
    if (env.GOOGLE_API_KEY) {
      try {
        const googleClient = new ChatGoogleGenerativeAI({
          model: env.GOOGLE_MODEL_NAME || 'models/gemini-2.0-flash-lite-preview-02-05',
          apiKey: env.GOOGLE_API_KEY, 
          temperature: 0.2,
          maxOutputTokens: 4096,
        });
        

        this.providers.push({
          name: 'google',
          model: env.GOOGLE_MODEL_NAME || 'models/gemini-2.0-flash-lite-preview-02-05',
          client: googleClient,
          isConfigured: true,
          priority: 1
        });

        logger.info('‚úÖ Google Gemini LLM configurado correctamente');
      } catch (error) {
        logger.warn('‚ùå Error configurando Google Gemini LLM:', error);
        this.providers.push({
          name: 'google',
          model: 'models/gemini-2.0-flash-lite-preview-02-05',
          client: null,
          isConfigured: false,
          priority: 1
        });
      }
    } else {
      logger.info('‚ö†Ô∏è Google Gemini no configurado (GOOGLE_API_KEY no encontrada)');
    }

    // OpenAI (Priority 2) - Solo si est√° configurado
    if (env.OPENAI_API_KEY) {
      try {
        const openaiClient = new ChatOpenAI({
          modelName: env.OPENAI_MODEL_NAME || 'gpt-4-turbo',
          apiKey: env.OPENAI_API_KEY,
          temperature: 0.2,
          maxTokens: 4096,
          timeout: env.OPENAI_API_TIMEOUT,
        });

        this.providers.push({
          name: 'openai',
          model: env.OPENAI_MODEL_NAME || 'gpt-4-turbo',
          client: openaiClient,
          isConfigured: true,
          priority: 2
        });

        logger.info('‚úÖ OpenAI LLM configurado correctamente');
      } catch (error) {
        logger.warn('‚ùå Error configurando OpenAI LLM:', error);
        this.providers.push({
          name: 'openai',
          model: 'gpt-4-turbo',
          client: null,
          isConfigured: false,
          priority: 2
        });
      }
    } else {
      logger.info('‚ö†Ô∏è OpenAI no configurado (OPENAI_API_KEY no encontrada)');
    }

    // DeepSeek (Priority 3) - Solo si est√° configurado
    if (env.DEEPSEEK_API_KEY) {
      try {
        const deepseekClient = new ChatOpenAI({
          modelName: env.DEEPSEEK_MODEL_NAME || 'deepseek-chat',
          apiKey: env.DEEPSEEK_API_KEY,
          configuration: {
            baseURL: env.DEEPSEEK_BASE_URL || 'https://api.deepseek.com/v1',
          },
          temperature: 0.2,
          maxTokens: 4096,
        });

        this.providers.push({
          name: 'deepseek',
          model: env.DEEPSEEK_MODEL_NAME || 'deepseek-chat',
          client: deepseekClient,
          isConfigured: true,
          priority: 3
        });

        logger.info('‚úÖ DeepSeek LLM configurado correctamente');
      } catch (error) {
        logger.warn('‚ùå Error configurando DeepSeek LLM:', error);
        this.providers.push({
          name: 'deepseek',
          model: 'deepseek-chat',
          client: null,
          isConfigured: false,
          priority: 3
        });
      }
    } else {
      logger.info('‚ö†Ô∏è DeepSeek no configurado (DEEPSEEK_API_KEY no encontrada)');
    }

    // Ordenar por prioridad y filtrar solo los configurados
    this.providers.sort((a, b) => a.priority - b.priority);
    const configuredProviders = this.providers.filter(p => p.isConfigured);

    if (configuredProviders.length === 0) {
      logger.error('üö® CR√çTICO: No hay proveedores LLM configurados! Verifica tus variables de entorno.');
      throw new Error('No LLM providers available - check your environment variables');
    }

    logger.info('üéØ LLM Manager inicializado exitosamente:', {
      totalProviders: this.providers.length,
      configuredProviders: configuredProviders.length,
      activeProviders: configuredProviders.map(p => ({ 
        name: p.name, 
        model: p.model, 
        priority: p.priority 
      })),
      fallbackStrategy: configuredProviders.length > 1 ? 'Habilitado' : 'Deshabilitado (solo 1 proveedor)'
    });
  }

  /**
   * Obtiene cliente LLM con fallback autom√°tico entre proveedores configurados
   */
  async getLLMClient(config?: LLMConfig): Promise<any> {
    const configuredProviders = this.providers.filter(p => p.isConfigured);
    
    if (configuredProviders.length === 0) {
      throw new Error('No LLM providers configured');
    }

    // Si solo hay un proveedor, √∫salo directamente
    if (configuredProviders.length === 1) {
      const provider = configuredProviders[0];
      logger.info(`üéØ Usando √∫nico proveedor disponible: ${provider.name}`);
      return this.configureClient(provider, config);
    }

    // Sistema de fallback para m√∫ltiples proveedores
    let lastError: Error | null = null;

    for (let attempt = 1; attempt <= this.maxRetries; attempt++) {
      for (const provider of configuredProviders) {
        try {
          logger.debug(`üîÑ Intentando ${provider.name} (intento ${attempt}/${this.maxRetries})`);
          
          // Test r√°pido del proveedor
          await this.testProvider(provider.client, config);
          
          logger.info(`‚úÖ Conectado exitosamente a ${provider.name} LLM`);
          return this.configureClient(provider, config);
          
        } catch (error) {
          lastError = error instanceof Error ? error : new Error('Unknown error');
          
          logger.warn(`‚ùå ${provider.name} fall√≥ (intento ${attempt}/${this.maxRetries}):`, {
            error: lastError.message,
            provider: provider.name,
            model: provider.model,
            willRetry: attempt < this.maxRetries || provider !== configuredProviders[configuredProviders.length - 1]
          });
          
          // Si no es el √∫ltimo proveedor o √∫ltimo intento, contin√∫a
          if (provider !== configuredProviders[configuredProviders.length - 1] || attempt < this.maxRetries) {
            continue;
          }
        }
      }
      
      // Esperar antes del siguiente ciclo de reintentos
      if (attempt < this.maxRetries) {
        logger.info(`‚è≥ Todos los proveedores fallaron en intento ${attempt}. Reintentando en ${this.retryDelay}ms...`);
        await this.sleep(this.retryDelay);
        this.retryDelay *= 2; // Backoff exponencial
      }
    }

    // Todos los proveedores fallaron despu√©s de todos los reintentos
    const error = new Error(`Todos los proveedores LLM fallaron despu√©s de ${this.maxRetries} intentos`);
    logger.error('üö® LLM Manager: Fallo completo', {
      attempts: this.maxRetries,
      providers: configuredProviders.map(p => p.name),
      lastError: lastError?.message
    });
    throw error;
  }

  /**
   * Prueba si un proveedor est√° funcionando
   */
  private async testProvider(client: any, config?: LLMConfig): Promise<void> {
    const configuredClient = this.configureBasicClient(client, config);
    
    // Mensaje de prueba simple
    const testResponse = await configuredClient.invoke([
      { role: 'user', content: 'Responde solo "OK"' }
    ]);
    
    if (!testResponse || !testResponse.content) {
      throw new Error(`Provider devolvi√≥ respuesta vac√≠a`);
    }

    // Validaci√≥n adicional del contenido
    if (typeof testResponse.content !== 'string' || testResponse.content.trim().length === 0) {
      throw new Error(`Provider devolvi√≥ contenido inv√°lido`);
    }
  }

  /**
   * Configura cliente con configuraci√≥n personalizada y soporte para structured output
   */
  private configureClient(provider: LLMProvider, config?: LLMConfig): any {
    if (!config) return provider.client;

    const client = provider.client;
    
    // Configuraci√≥n b√°sica del cliente
    const configuredClient = this.configureBasicClient(client, config);
    
    // Si se requiere structured output, configurarlo seg√∫n el proveedor
    if (config.structuredOutput && config.outputSchema) {
      return this.addStructuredOutput(provider, configuredClient, config.outputSchema);
    }
    
    return configuredClient;
  }

  /**
   * Configuraci√≥n b√°sica del cliente (temperatura, tokens, etc.)
   */
  private configureBasicClient(client: any, config?: LLMConfig): any {
    if (!config) return client;
    
    return client.bind({
      temperature: config.temperature ?? client.temperature,
      maxTokens: config.maxTokens ?? client.maxTokens,
      timeout: config.timeout ?? client.timeout,
    });
  }

  /**
   * A√±ade soporte para structured output seg√∫n el proveedor
   */
  private addStructuredOutput(provider: LLMProvider, client: any, outputSchema: any): any {
    if (provider.name === 'openai') {
      // OpenAI tiene soporte nativo para structured output
      return client.withStructuredOutput(outputSchema, {
        method: "tool_calling",
      });
    } else if (provider.name === 'google') {
      // Google Gemini requiere un enfoque diferente usando JsonOutputParser
      const parser = new JsonOutputParser();
      
      // Crear un prompt que instruya al modelo a devolver JSON v√°lido
      const structuredPrompt = PromptTemplate.fromTemplate(`
{input}

IMPORTANTE: Tu respuesta debe ser un JSON v√°lido que siga exactamente este esquema:
{schema}

Responde √öNICAMENTE con el JSON, sin texto adicional antes o despu√©s.
`);
      
      // Crear una cadena que combine el prompt estructurado con el parser
      return structuredPrompt
        .pipe(client)
        .pipe(parser)
        .bind({
          schema: JSON.stringify(outputSchema.parameters || outputSchema, null, 2)
        });
    } else if (provider.name === 'deepseek') {
      // DeepSeek usa la misma API que OpenAI, pero puede no tener structured output
      // Intentamos primero con structured output, si falla usamos JSON parser
      try {
        return client.withStructuredOutput(outputSchema, {
          method: "tool_calling",
        });
      } catch (error) {
        logger.warn('DeepSeek no soporta structured output nativo, usando JSON parser');
        const parser = new JsonOutputParser();
        
        const structuredPrompt = PromptTemplate.fromTemplate(`
{input}

IMPORTANTE: Tu respuesta debe ser un JSON v√°lido que siga exactamente este esquema:
{schema}

Responde √öNICAMENTE con el JSON, sin texto adicional antes o despu√©s.
`);
        
        return structuredPrompt
          .pipe(client)
          .pipe(parser)
          .bind({
            schema: JSON.stringify(outputSchema.parameters || outputSchema, null, 2)
          });
      }
    }
    
    // Fallback para otros proveedores
    logger.warn(`Structured output no implementado para ${provider.name}, usando JSON parser gen√©rico`);
    const parser = new JsonOutputParser();
    
    const structuredPrompt = PromptTemplate.fromTemplate(`
{input}

IMPORTANTE: Tu respuesta debe ser un JSON v√°lido que siga exactamente este esquema:
{schema}

Responde √öNICAMENTE con el JSON, sin texto adicional antes o despu√©s.
`);
    
    return structuredPrompt
      .pipe(client)
      .pipe(parser)
      .bind({
        schema: JSON.stringify(outputSchema.parameters || outputSchema, null, 2)
      });
  }

  /**
   * M√©todo de conveniencia para obtener cliente con structured output
   */
  async getStructuredLLMClient(outputSchema: any, config?: Omit<LLMConfig, 'structuredOutput' | 'outputSchema'>): Promise<any> {
    return await this.getLLMClient({
      ...config,
      structuredOutput: true,
      outputSchema
    });
  }

  /**
   * Obtiene estado de proveedores para monitoreo
   */
  getProviderStatus(): { name: string; configured: boolean; priority: number; model: string }[] {
    return this.providers.map(p => ({
      name: p.name,
      configured: p.isConfigured,
      priority: p.priority,
      model: p.model
    }));
  }

  /**
   * Obtiene resumen de configuraci√≥n actual
   */
  getConfigurationSummary(): {
    totalProviders: number;
    configuredProviders: number;
    activeProviders: string[];
    fallbackEnabled: boolean;
    primaryProvider: string | null;
  } {
    const configuredProviders = this.providers.filter(p => p.isConfigured);
    
    return {
      totalProviders: this.providers.length,
      configuredProviders: configuredProviders.length,
      activeProviders: configuredProviders.map(p => p.name),
      fallbackEnabled: configuredProviders.length > 1,
      primaryProvider: configuredProviders.length > 0 ? configuredProviders[0].name : null
    };
  }

  /**
   * Fuerza actualizaci√≥n de proveedores (√∫til para cambios de configuraci√≥n en runtime)
   */
  async refreshProviders(): Promise<void> {
    logger.info('üîÑ Actualizando proveedores LLM...');
    this.providers = [];
    this.retryDelay = 1000; // Reset retry delay
    this.initializeProviders();
  }

  private sleep(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }
}

// Exportar instancia singleton
export const llmManager = new LLMManager();

// Exportar funci√≥n de conveniencia
export async function getLLMClient(config?: LLMConfig): Promise<any> {
  return await llmManager.getLLMClient(config);
}